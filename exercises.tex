\input{header}

\newcommand\ProgrammingExercise{This is a programming exercise. For the relevant code please see \href{\RepoAddress{}}{the repo}.}


\begin{document}

{\huge Exercises} \hfill {\huge Reinforcement Learning: An Introduction}\\
\Rule\\
\tableofcontents
\mbox{}\\
\Rule
\mbox{}\\


\setcounter{secnumdepth}{2}
% You can use the exam package to make some of this a bit easier.

\clearpage
\vfill
\thispagestyle{empty}
\begin{center}
    \emph{Code for exercises can be found at \href{\RepoAddress{}}{github.com/brynhayder/reinforcement\_learning\_an\_introduction}}\\[2cm]
Note that equation numbers in questions will refer to the original text.
\vfill
\end{center}
\clearpage
\section{Introduction}
\subsection{Exercise 1.1: Self-Play}
\subsubsection{Q}
Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?

\subsubsection{A}
\begin{itemize}
    \item Would learn a different policy than playing a fixed opponent since the opponent would also be changing in this case.
    \item May not be able to learn an optimal strategy as the opponent keeps changing also.
    \item Could get stuck in loops.
    \item Policy could remain static since on average they would draw each iteration.
\end{itemize}

\subsection{Exercise 1.2: Symmetries}
\subsubsection{Q}
Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?

\subsubsection{A}
\begin{itemize}
    \item We could label the states as unique up to symmetries so that our search space is smaller, this way we will get a better estimate of optimal play.
    \item If we are playing an opponent who does not take symmetries into account when they are playing then we should not label the states as the same since the opponent is part of the environment and the environment is not the same in those states.
\end{itemize}

\subsection{Exercise 1.3: Greedy Play}
\subsubsection{Q}
Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur

\subsubsection{A}
\begin{itemize}
    \item The greedy player will not explore, so will in general perform worse than the non-greedy player
    \item If the greedy player had a perfect estimate of the value of states then this would be fine.
\end{itemize}

\subsection{Exercise 1.4: Learning from Exploration}
\subsubsection{Q}
Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?
\subsubsection{A}
I think that an estimate for the probability of the state producing a win should be based on the optimal moves from that state.
\begin{itemize}
    \item The one in which we only record the optimal moves is the probability of our optimal agent winning. If we include exploration then this is the probability of the training agent winning.
    \item Better to learn the probability of winning with no exploration since this is how the agent will perform in real time play.
    \item Updating from optimal moves only will increase probability of winning.
\end{itemize}

\subsection{Exercise 1.5: Other Improvements}
\subsubsection{Q}
Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?

\subsubsection{A}
I'm not too sure here...
\begin{itemize}
    \item We could rank the draws as better than the losses.
    \item We might like to try running multiple iterations of games before updating our weights as this might give a better estimate.
\end{itemize}


\clearpage
\section{Multi-armed Bandits}
\subsection{Exercise 2.1}
\subsubsection{Q}
In $\varepsilon$-greedy action selection, for the case of two actions and $\varepsilon = 0.5$, what is the probability that the greedy action is selected?

\subsubsection{A}
0.5.


\subsection{Exercise 2.2: Bandit example}
\subsubsection{Q}
Consider a $k$-armed bandit problem with $k = 4$ actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\varepsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1$, $R_1 =1$, $A_2 =2$, $R_2 =1$, $A_3 =2$, $R_3 =2$, $A_4 =2$, $R_4 =2$, $A_5 =3$, $R_5 =0$. On some of these time steps the $\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?

\subsubsection{A}
$A_2$ and $A_5$ were definitely exploratory. Any of the others \emph{could} have been exploratory.


\subsection{Exercise 2.3}
\subsubsection{Q}
In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.

\subsubsection{A}
The $\varepsilon = 0.01$ will perform better because in both cases as $t \to \infty$ we have $Q_t \to q_*$. The total reward and probability of choosing the optimal action will therefore be 10 times larger in this case than for $\varepsilon = 0.1$.


\subsection{Exercise 2.4}
\label{ex:2.4}
\subsubsection{Q}
If the step-size parameters, $\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?

\subsubsection{A}
Let $\alpha_0 = 1$, then 
\begin{equation}
    Q_{n + 1} = \left(\prod_{i=1}^n (1 - \alpha_i) \right) Q_1 + \sum_{i = 1}^{n}  \alpha_{i} R_{i} \prod_{k = i + 1}^n
(1 - \alpha_k).
\end{equation}
Where $\prod_{i=x}^y f(i) \doteq 1$ if $x > y$.

\subsection{Exercise 2.5 (programming)}
\subsubsection{Q}
Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for non-stationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\alpha$ = 0.1. Use $\varepsilon$ = 0.1 and longer runs, say of 10,000 steps.

\subsubsection{A}
\ProgrammingExercise

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_5/learning_curve.png}


\subsection{Exercise 2.6: Mysterious Values}
\subsubsection{Q}
The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?

\subsubsection{A}
At some point after step 10, the agent will find the optimal value. It will then choose this value greedily. The small step-size parameter (small relative to the initialisation value of 5) means that the estimate of the optimal value will converge slowly towards its true value.\\

It is likely that this true value is less than 5. This means that, due to the small step size, one of the sub-optimal actions will still have a value close to 5. Thus, at some point, the agent begins to act sub-optimally again.

\subsection{Exercise 2.7: Unbiased Constant Step Trick}
\subsubsection{Q}
In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis in (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on non-stationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on non-stationary problems? One way is to use a step size of
\begin{equation}
    \beta_t \doteq \alpha / \bar{o}_t,
\end{equation}
where $\alpha > 0$ is a conventional constant step size and $\bar{o}_t$ is a trace of one that starts at 0:
\begin{equation}
    \bar{o}_{t+1} = \bar{o}_t + \alpha (1 - \bar{o}_t)
\end{equation}
for $t \geq 1$ and with $\bar{o}_1 \doteq \alpha$.\\

Carry out an analysis like that in (2.6) to show that $\beta_t$ is an exponential recency-weighted average \emph{without initial bias}. 

\subsubsection{A}
Consider the answer to \hyperref[ex:2.4]{Exercise 2.4}. There is no dependence of $Q_k$ on $Q_1$ for $k > 1$ since $\beta_1 = 1$. Now it remains to show that the weights in the remaining sum decrease as we look further into the past. That is
\begin{equation}
    w_i = \beta_i \prod_{k = i + 1}^{n} (1 - \beta_k)
\end{equation}
increases with $i$ for fixed n. For this, observe that
\begin{equation}
    \frac{w_{i+1}}{w_i} = \frac{\beta_{i+1}}{\beta_i(1 - \beta_{i + 1})} = \frac{1}{1 - \alpha} > 1
\end{equation} 
where we have assumed $\alpha < 1$. If $\alpha = 1$ then $\beta_t = 1 \,\, \forall \, t$.

\subsection{Exercise 2.8: UCB Spikes}
\subsubsection{Q}
In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if $c = 1$, then the spike is less prominent.

\subsubsection{A}
In the first 10 steps the agent cycles through all of the actions because when $N_t(a) = 0$ then $a$ is considered maximal. On the 11th step the agent will most often then choose greedily. The agent will continue to choose greedily until $\mathrm{ln}(t)$ overtakes $N_t(a)$ for one of the other actions, in which case the agent begins to explore again hence reducing rewards.\\

Note that, in the long run, $N_t = O(t)$ and $\mathrm{ln}(t) / t \to 1$. So this agent is `asymptotically greedy'.


\subsection{Exercise 2.9}
\subsubsection{Q}
Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.

\subsubsection{A}
Let the two actions be denoted by 0 and 1. Now
\begin{equation}
    \P{}(A_t = 1) = \frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(0)}} = \frac{1}{1 + e^{-x}}, 
\end{equation}
where $x = H_t(1) - H_t(0)$ is the relative preference of 1 over 0.

\subsection{Exercise 2.10}
\subsubsection{Q}
Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?

\subsubsection{A}
I assume the rewards are stationary.\\

One should choose the action with the highest expected reward. In the first case, both action 1 and 2 have expected value of 0.5, so it doesn't matter which you pick.\\

In the second case one should run a normal bandit method separately on each colour. The expected reward from identifying the optimal actions in each case is 0.55.

\subsection{Exercise 2.11 (programming)}
\subsubsection{Q}
Make a figure analogous to Figure 2.6 for the non-stationary case outlined in Exercise 2.5. Include the constant-step-size $\varepsilon$-greedy algorithm with $\alpha=0.1$. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.

\subsubsection{A}
\ProgrammingExercise

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_11/action_values.png}

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_11/parameter_study.png}


\clearpage
\section{Finite Markov Decision Processes}

\subsection{Exercise 3.1}
\subsubsection{Q}
Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as \emph{different} from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.

\subsubsection{A}
\begin{enumerate}
    \item Simple example is a robot that hoovers a room. The state can be how much dust there is on the ground and where the robot is (including it's orientation). Actions can be to move and hoover. The reward can be the amount by which it reduces dust in the room on that action. This is Markov because all that is important from the previous state to the future is where the dust is left (maybe also how much).
    \item Outlandish example is a football coach. Actions are playing strategies. Rewards are goals. State is current score, team fitness, etc..
    \item financial trader. State is their current holdings on an asset. Reward is money from a trade. Actions are buy/sell. Maybe not markov because the environment may change predictably based on information from multiple steps ago.
\end{enumerate}

\subsection{Exercise 3.2}
\subsubsection{Q}
Is the MDP framework adequate to usefully represent \emph{all} goal-directed learning tasks? Can you think of any clear exceptions?

\subsubsection{A}
The main thing about the MDP is that Markov property. There are tasks where this does not hold. For instance, in Poker, the previous states will determine what is in the deck and what is not. This does not obey Markov property.

\subsection{Exercise 3.3}
\subsubsection{Q}
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of \emph{where} to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

\subsubsection{A}

\begin{itemize}
    \item The natural distinction depends on the task. If the task is to go from one location to another, the actions might be in terms of directing the car and altering its speed. 
    \item This also depends on what we consider to be the decision making part here. If we consider the decisions to be made by the brain of a human driving, then their physical body will form part of the environment -- if they break their leg then it will effect their goal.
    \item Actions have to be things that the agent can actually control. Take the example of an autonomous vehicle. One might consider that the agent has complete control over the car's break, accelerator and steering wheel. These operations would form the actions for the agent. 
\end{itemize}


\subsection{Exercise 3.4}
\subsubsection{Q}
Give a table analogous to the one in Example 3.3, but for $p(s', r| s, a)$. It should have columns $s, a, s', r$, and a row for every 4-tuple for which $p(s', r | s, a) > 0$.

\subsubsection{A}
\begin{table}[h!]
\centering
\begin{tabular}{ll|ll|c}
    $s$ & $a$ & $s'$ & $r$ & $p(s', r | s, a)$ \\
    \hline
     \texttt{high}& \texttt{search}&  \texttt{high}&  $r_{\texttt{search}}$& $\alpha$  \\
     \texttt{high}& \texttt{search}&  \texttt{low}&   $r_{\texttt{search}}$& $1 - \alpha$ \\
     \texttt{high}& \texttt{wait}&  \texttt{high}&   $r_{\texttt{wait}}$&  $1$ \\
     \texttt{low}& \texttt{recharge}&   \texttt{high}&    $0$&   $1$\\
     \texttt{low}& \texttt{search}&  \texttt{high}&  $-3$& $1 - \beta$  \\
     \texttt{low}& \texttt{search}&  \texttt{low}&   $r_{\texttt{search}}$& $\beta $\\
     \texttt{low}& \texttt{wait}&  \texttt{low}&   $r_{\texttt{wait}}$&  $1$
\end{tabular}


\caption{Transition table}
\label{table:3.4}
\end{table}


\subsection{Exercise 3.5}
\subsubsection{Q}
The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3)

\subsubsection{A}
\begin{equation}
    \sum_{s' \in \mathcal{S}^{+}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1
\end{equation}


\subsection{Exercise 3.6}
\subsubsection{Q}
Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $-1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

\subsubsection{A}
Note that the pole will fall eventually with probability 1
\begin{equation}
    G_t = - \gamma^{T - t}.
\end{equation}
Whereas in the continuing case the value is
\begin{equation}
    G_t = - \sum_{k \in \mathcal{K}} \gamma^{k - t},
\end{equation}
where $\mathcal{K}$ is the set of times after $t$ at which the pole falls over.

\subsection{Exercise 3.7}
\subsubsection{Q}
Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes--the successive runs through the maze--so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

\subsubsection{A}
If the agent keeps going randomly, it will reach the end of the maze with probability 1, so the value of $G$ under most strategies is $1$. What you actually want is for the thing to leave the maze as quickly as possible.\\

Note also that there are some instances in which the agent might just get stuck in a loop. You would have to impose another rule to put the agent into a terminal state here.

\subsection{Exercise 3.8}
\subsubsection{Q}
Suppose $\gamma = 0.5$ and the following sequence of rewards is received $R_1=-1$, $R_2 =2$,$R_3 =6$,$R_4 =3$, and $R_5 =2$, with $T =5$. What are $G_0$, $G_1$, $\dots$, $G_5$? Hint: Work backwards.

\subsubsection{A}
$G_0 = 2$, $G_1 = 3$, $G_2 = 2$, $G_3 = \frac12$, $G_4 = \frac18$, $G_5=0$.

\subsection{Exercise 3.9}
\subsubsection{Q}
Suppose $\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$s. What are $G_1$ and $G_0$?
\subsubsection{A}
\begin{align}
    G_1 &= 7 \frac{\gamma}{1 - \gamma} \\
    G_0 &= 2 + 7 \frac{\gamma}{1 - \gamma}
\end{align}


\subsection{Exercise 3.10}
\subsubsection{Q}
Prove (3.10).

\subsubsection{A}
Take $r \in \mathbb{C}$ with $|r| < 1$, then
\begin{align*}
    S_N &\doteq \sum_{i=0}^N r^i \\
    rS_N - S_N &= r^{N+1} - 1 \\
    S_N &= \frac{1 - r^{N+1}}{1 - r} \\
    S &\doteq \lim_{N \to \infty} S_N = \frac{1}{1 - r}
\end{align*}

\subsection{Exercise 3.11}
\subsubsection{Q}
If the current state is $S_t$, and actions are selected according to stochastic policy $\pi$, then what is the expectation of $R_{t+1}$ in terms of $\pi$ and the four-argument function $p$ (3.2)?

\subsubsection{A}
\begin{equation}
    \Epi{} [R_{t+1} | S_t = s] = \sum_{a} \pi(a | s) \sum_{s', r} r p(s', r | s, a)
\end{equation}


\subsection{Exercise 3.12}
\subsubsection{Q}
The Bellman equation (3.14) must hold for each state for the value function $v_\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at $+0.7$, with respect to its four neighbouring states, valued at $+2.3$, $+0.4$, $-0.4$, and $+0.7$. (These numbers are accurate only to one decimal place.)

\subsubsection{A}
\begin{align*}
    v_\pi(\mathtt{center}) &\doteq \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')] \\
    & = \frac14 \times 0.9 \times \sum_{s'} v_\pi(s')\\
    & = \frac14 \times 0.9 \times 3.0 \\
    & = 0.675
\end{align*}


\subsection{Exercise 3.13}
\subsubsection{Q}
What is the Bellman equation for action values, that is, for $q_\pi$? It must give the action value $q_\pi(s, a)$ in terms of the action values, $q_\pi(s',a')$, of possible successors to the state–action pair $(s,a)$. Hint: the backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.

\subsubsection{A}
\begin{align*}
    q_\pi &\doteq \Epi[G_t | S_t=s, A_t=a]\\
          &= \Epi[R_{t+1}| S_t=s, A_t=a] + \gamma \Epi[G_{t+1} |S_t = s, A_t = a] \\
          &= \sum_{s', r} p(s', r|s, a)r + \gamma \sum_{s', r} p(s', r|s, a) \sum_{a'}\pi(a'|s')\Epi[G_{t+1}| S_{t+1}=s', A_{t+1}=a'] \\
          &= \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'}\pi(a'|s')q_\pi(s', a')]
\end{align*}


\subsection{Exercise 3.14}
\subsubsection{Q}
In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of $c$ and $\gamma$?

\subsubsection{A}
We choose actions by relative (additive) values. Add $c$ to all states and 
\begin{equation}
    G_t \mapsto G_t + \frac{c}{1 - \gamma}
\end{equation}
so relative values unchanged.\\

$v_c = \frac{c}{1 - \gamma}$

\subsection{Exercise 3.15}
\subsubsection{Q}
Now consider adding a constant $c$ to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.

\subsubsection{A}
Let terminal time be $T$. In this case we have 
\begin{equation}
    G_t \mapsto G_t + c\frac{1 - \gamma^T}{1 - \gamma},
\end{equation}
so if the agent can procrastinate termination, then all else being equal it will increase $v_\pi$.\\

Suppose that we have an episodic task with one state $S$ and two actions $A_0, A_1$. $A_0$ takes agent to terminal state with reward $1$, while $A_1$ takes agent back to $S$ with reward $0$. In this case the agent should terminate to maximise reward. \\ 

If we add $1$ to each reward then the return for doing $A_1$ forever is $\frac{1}{1-\gamma}$. Which can be bigger than $2$ if we choose a discount factor smaller than $\frac12$.

\subsection{Exercise 3.16}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_16_question.png}
 
\subsubsection{A}
\begin{align*}
    v_\pi(s) & = \Epi[q_\pi(S_t, A_t) |S_t = s, A_t = a] \\ 
          & = \sum_a \pi(a|s) q_\pi(s, a)
\end{align*}

\subsection{Exercise 3.17}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_17_question.png}
 
\subsubsection{A} 
\begin{align*}
    q_\pi(s, a) & = \Epi[R_{t+1} + v_\pi(s') |S_t = s, A_t = a] \\ 
          & = \sum_{s', r} p(s', r|s, a) [r + v_\pi(s')]
\end{align*}


\subsection{Exercise 3.18}
\subsubsection{Q}
Draw or describe the optimal state-value function for the golf example.

\subsubsection{A}
(Using the pictures.) Optimal state value gives values according to \texttt{driver} when off the green, then according to \texttt{putter} on the green.\\

Optimal policy is to use \texttt{driver} when off green and \texttt{putter} when on green.
 
 
\subsection{Exercise 3.19}
\subsubsection{Q}
 Draw or describe the contours of the optimal action-value function for putting, $q_*(s, \mathtt{putter})$, for the golf example.

\subsubsection{A}
Should be the same as the value for the policy that always uses the \texttt{putter}.

\includegraphics[width=\textwidth]{data/exercise_output/ex_3_19/ex_3_19_answer.png}


\subsection{Exercise 3.20}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_20_question.png}
 
\subsubsection{A}
When $\gamma=0$, $v_{\pi_{\mathtt{left}}}$ is optimal. When $\gamma = 0.5$, they are both optimal. When $\gamma=0.9$, $v_{\pi_{\mathtt{right}}}$ is optimal.

\subsection{Exercise 3.21}
\subsubsection{Q}
Give the Bellman equation for $q_*$ for the recycling robot.

\subsubsection{A}
This is just writing out the equation below, filling in the values given in the robot example.
\begin{equation}
    q_*(s, a) = \sum_{s', r} p(s', r | s, a)[r + \gamma \max_{a'} q_*(s', a')]
\end{equation}

\subsection{Exercise 3.22}
\subsubsection{Q}
Figure 3.5 gives the optimal value of the best state of the gridworld as $24.4$, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.

\subsubsection{A}
All actions tak the agent to $A'$ with reward $10$. We can see that $\gamma = 16.0 / 17.8 = 0.9$. This means that
\begin{equation}
    v = 10 + 16 \times 0.9 = 24.4.
\end{equation}
This is using the following framework
\begin{equation}
    v_*(s) = \max_{a}\sum_{s', r} p (s', r| s, a)[r + \gamma v_*(s')].
\end{equation}


\subsection{Exercise 3.23}
\subsubsection{Q}
Give an equation for $v_*$ in terms of $q_*$

\subsubsection{A}
\begin{equation}
    v_*(s) = \sum_a \pi^*(a | s) q_*(s, a)
\end{equation}


\subsection{Exercise 3.24}
\subsubsection{Q}
Give an equation for $q_*$ in terms of $v_*$ and the world's dynamics $p(s', r| s, a)$.

\subsubsection{A}
\begin{align}
    q_* (s, a) &= \mathbb{E} [ R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &= \sum_{s', r} (r + \gamma v_*(s'))p(s', r | s, a)
\end{align} 


\subsection{Exercise 3.25}
\subsubsection{Q}
Give an equation for $\pi_*$ in terms of $q_*$.

\subsubsection{A}
This is just any policy that acts greedily w.r.t. the optimal action-value function.
\begin{equation}
    \pi_* (a|s) = \frac{\mathds{1}\{a = \argmax_{a'} q_*(a', s)\}}{\sum_{a} \mathds{1}\{a = \argmax_{a'}q_*(a', s)\}}
\end{equation} 

\subsection{Exercise 3.26}
\subsubsection{Q}
Give an equation for $\pi_*$ in terms of $v_*$ and the world's dynamics $p(s', r| s, a)$.

\subsubsection{A}
This is just the answer to 3.25 with the answer to 3.24 substituted in for $q_*$.



\clearpage
\section{Dynamic Programming}

\subsection{Exercise 4.1}
\subsubsection{Q}
In Example 4.1, if $\pi$ is the equiprobable random policy, what is $q_\pi(11, \mathtt{down})$? What is $q_\pi(7, \mathtt{down})$?

\subsubsection{A}
$q_\pi(11, \mathtt{down}) = -1$ since goes to terminal state. $q_\pi(7, \mathtt{down}) = -15$.

\subsection{Exercise 4.2}
\subsubsection{Q}
In Example 4.1, suppose a new state $15$ is added to the gridworld just below state $13$, and its actions, \texttt{left}, \texttt{up}, \texttt{right}, and \texttt{down}, take the agent to states $12$, $13$, $14$, and $15$, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of state $13$ are also changed, such that action down from state $13$ takes the agent to the new state $15$. What is $v_\pi(15)$ for the equiprobable random policy in this case?

\subsubsection{A}
$v_\pi(15) = -20$ if dynamics unchanged. If dynamics changed then apparently the state value is the same, but you would need to verify Bellman equations for all states for this.

\subsection{Exercise 4.3}
\subsubsection{Q}
What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\pi$ and its successive approximations by a sequence of functions $q_0, q_1, q_2, \dots$?

\subsubsection{A}
\begin{equation}
    q_{k+1}(s, a) = \sum_{s', r} p(s', r | s, a)\left[r + \gamma \sum_{a'} \pi(a'|s)q_k(s', a')\right]
\end{equation}

\subsection{Exercise 4.4}
\subsubsection{Q}
The policy iteration algorithm on the previous page has a subtle bug in that it may never terminate if the policy continually switches between two or more policies that are equally good. This is ok for pedagogy, but not for actual use. Modify the pseudocode so that convergence is guaranteed.

\subsubsection{A}
One problem is that the $\argmax_a$ has ties broken arbitrarily, this means that the same value function can give rise to different policies.\\

The way to solve this is to change the algorithm to take the whole set of maximal actions on each step and see if this set is stable and see if the policy is stable with respect to choosing actions from this set.


\subsection{Exercise 4.5}
\subsubsection{Q}
How would policy iteration be defined for action values? Give a complete algorithm for computing $q_*$, analogous to that on page 80 for computing $q_*$. Please pay special attention to this exercise, because the ideas involved will be used throughout the rest of the book.

\subsubsection{A}
We know that
\begin{equation}
    v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a|s)q_\pi(s, a)
\end{equation}
so we know that
\begin{equation}
    q_\pi(s, \pi'(s)) \geq \sum_{a \in \mathcal{A}(s)} \pi(a|s)q_\pi(s, a)
\end{equation}
if $\pi'$ is greedy with respect to $\pi$. So we know the algorithm still works for action values.\\

All there is now is to substitute the update for the action-value update and make the policy greedy with respect to the last iteration's action-values. Also need to make sure that the $\argmax_a$ is done consistently.

\subsection{Exercise 4.6}
\subsubsection{Q}
Suppose you are restricted to considering only policies that are $\varepsilon$-soft, meaning that the probability of selecting each action in each state, $s$, is at least $\varepsilon / |\mathcal{A}(s)|$. Describe qualitatively the changes that would be required in each of the steps 3, 2, and 1, in that order, of the policy iteration algorithm for $v_\pi$ (page 80).

\subsubsection{A}
\begin{enumerate}
    \item No change (but need policy to be able to be stochastic of course)
    \item Need to re-write the Bellman update $v(s) \longleftarrow \sum_{a \in \mathcal{A}(s)} \pi(a|s)\sum_{s', r}p(s', r|s, a)\left[ r + \gamma v(s') \right]$
    \item Construct a greedy policy that puts weight on the greedy actions but is $\varepsilon$-soft. Be careful with the consistency of the $\argmax$.
\end{enumerate}

\subsection{Exercise 4.7 (programming): Jack's Car Rental}

\includegraphics[width=\textwidth]{data/exercise_questions/jacks_car_rental_example.png}

First we reproduce the original results.

\includegraphics[width=\textwidth]{data/exercise_output/ex_4_7/jacks_car_rental/jacks_car_rental.png}

\subsubsection{Q}
Write a program for policy iteration and re-solve Jack’s car rental problem with the following changes. One of Jack’s employees at the first location rides a bus home each night and lives near the second location. She is happy to shuttle one car to the second location for free. Each additional car still costs \$$2$, as do all cars moved in the other direction. In addition, Jack has limited parking space at each location. If more than $10$ cars are kept overnight at a location (after any moving of cars), then an additional cost of \$$4$ must be incurred to use a second parking lot (independent of how many cars are kept there). These sorts of nonlinearities and arbitrary dynamics often occur in real problems and cannot easily be handled by optimisation methods other than dynamic programming. To check your program, first replicate the results given for the original problem. If your computer is too slow for the full problem, cut all the numbers of cars in half.

\subsubsection{A}
\ProgrammingExercise\\
\includegraphics[width=\textwidth]{data/exercise_output/ex_4_7/altered_car_rental.png}

\subsection{Exercise 4.8}
\subsubsection{Q}
Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?

\subsubsection{A}
Since the coin is biased against us, we want to minimize the number of flips that we take. At 50 we can win with probability 0.4. At 51 if we bet small then we can get up to 52, but if we lose then we are still only back to 50 and we can again with with probability 0.4. (There is a whole paper on this problem called how to gamble if you must.)


\subsection{Exercise 4.9 (programming): Gambler's Problem}
\subsubsection{Q}
Implement value iteration for the gambler's problem and solve it for $p_h = 0.25$ and $p_h = 0.55$. In programming, you may find it convenient to introduce two dummy states corresponding to termination with capital of $0$ and $100$, giving them values of $0$ and $1$ respectively. Show your results graphically, as in Figure 4.3. Are your results stable as $\theta \to 0$?

\subsubsection{A}
\ProgrammingExercise\\

The process was stable as $\theta \to 0$ for $\P{}(\mathtt{win}) < 0.5$.

\includegraphics[width=\textwidth]{data/exercise_output/ex_4_9/values_and_policy_pwin_25.eps}

\includegraphics[width=\textwidth]{data/exercise_output/ex_4_9/values_and_policy_pwin_55.eps}

\subsection{Exercise 4.10}
\subsubsection{Q}
What is the analog of the value iteration update (4.10) for action values, $q_{k+1}(s, a)$?

\subsubsection{A}
\begin{equation}
    q_{k+1} = \max_{a'} \sum_{s', r} p(s', r| s, a)\left[r + \gamma q_k(s', a')\right]
\end{equation}


\clearpage
\section{Monte-Carlo Methods}
\subsection{Exercise 5.1}
\subsubsection{Q}
Consider the diagrams on the right in Figure 5.1. Why does the estimated value function jump up for the last two rows in the rear? Why does it drop off for the whole last row on the left? Why are the frontmost values higher in the upper diagrams than in the lower?

\subsubsection{A}
\begin{itemize}
    \item Policy is to hit unless $S \geq 20$. So you run a rik of going bust if you have 12-19, but you most likely win when you stick on 20 or 21
    \item Drops off because dealer has a usable ace
    \item Frontmost higher because you're less likely to go bust, but you still might get to 20 or 21 ($\pi$ always hits here).
\end{itemize}

\subsection{Exercise 5.2}
\subsubsection{Q}
Suppose every-visit MC was used instead of first-visit MC on the blackjack task. Would you expect the results to be very different? Why or why not?

\subsubsection{A}
Results would be the same because this game is memoryless (cards are drawn with replacement).

\subsection{Exercise 5.3}
\subsubsection{Q}
What is the backup diagram for Monte Carlo estimation of $q_\pi$?

\subsubsection{A}
The same as the one shown in the book for state valus, only we have state-action pairs instead of states.

\subsection{Exercise 5.4}
\subsubsection{Q}
What is the equation analogous to (5.6) for \emph{action} values $Q(s, a)$ instead of state values $V(s)$, again given returns generated using $b$?
\subsubsection{A}
We condition on taking action $a$ in state $s$.
\[
    q_\pi(s, a) = \Epi{}[\rho_{t+1:T-1} G_t | S_t = s, A_t = s]
\]
with returns generated from $b$. We estimate this quantity by
\[
    Q(s, a) = \frac{\sum_{t \in \mathcal{T}(s, a)} \rho_{t+1:T-1} G_t}{\sum_{t \in \mathcal{T}(s, a)} \rho_{t+1:T-1}}
\]
where $\mathcal{T}(s, a)$ now contains timestamps of visits to state-action pairs.

\subsection{Exercise 5.5}
\subsubsection{Q}
In learning curves such as those shown in Figure 5.3 error generally decreases with training, as indeed happened for the ordinary importance-sampling method. But for the weighted importance-sampling method error first increased and then decreased. Why do you think this happened?

\subsubsection{A}
When there are fewer episodes the importance sampling ratios will be zero with higher probability since the behaviour policy will stick on values smaller than 20 (since it is random). Zero happens to be close to $v_\pi(s)$.\\

This effect lessens as we get more diversity in the episode trajectories.\\

Then after this the error reduces because the variance in the estimator reduces.

\subsection{Exercise 5.6}
\subsubsection{Q}
The results with Example 5.5 and shown in Figure 5.4 used a first-visit MC method. Suppose that instead an every-visit MC method was used on the same problem. Would the variance of the estimator still be infinite? Why or why not?
\subsubsection{A}
Yes, all terms in the sum are $\geq 0$ and there woud just be more of them.

\subsection{Exercise 5.7}
\subsubsection{Q}
Modify the algorithm for first-visit MC policy evaluation (Section 5.1) to use the incremental implementation for sample averages described in Section 2.4.

\subsubsection{A}
Algo is the same apart from 
\begin{itemize}
    \item Initialise $V(s) = 0 \quad \forall s \in S$
    \item Don't need \emph{Returns(s)} lists.
    \item Remove the last two lines and put in \[ V(S_t) \leftarrow V(S_t) + \frac{1}{T- t}[ G_t - V(S_t) ] \]
\end{itemize}

\subsection{Exercise 5.8}
\subsubsection{Q}
Derive the weighted-average update rule (5.8) from (5.7). Follow the pattern of the derivation of the unweighted rule (2.3).

\subsubsection{A}
Have $C_0 = 0$, $C_n = \sum_{k = 1}^n W_k$ and 
\[
    V_{n+1} = \frac{\sum_{k = 1}^n W_kG_k}{C_n}.
\]
Therefore,
\begin{align}
    C_n V_{n+1} &= \sum_{k+1}^{n-1}W_kG_k + W_kG_k\\
                &= C_{n-1}V_n + W_nG_n \\
                &= (C_n - W_n)V_n + W_nG_n.
\end{align}
Finally
\[
    V_{n+1} = V_n + \frac{W_n}{C_n}[G_n - V_n].
\]

\subsection{Exercise 5.9}
\subsubsection{Q}
In the boxed algorithm for off-policy MC control, you may have been expecting the W update to have involved the importance-sampling ratio $\pi(A_t|S_t)$, but instead it involves $1/b(A_t|S_t)$. Why is this nevertheless correct?

\subsubsection{A}
$\pi$ is greedy, so 
\[
    \pi(a | s) = \mathds{1}\{a = \argmax_{a'} Q(s, a')\}.
\]

\subsection{Exercise 5.10 (programming): Racetrack}
\subsubsection{Q}
Consider driving a race car around a turn like those shown in Figure 5.5. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, −1, or 0 in each step, for a total of nine (3 × 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are −1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).
\subsubsection{A}
\ProgrammingExercise\\
\includegraphics[width=\textwidth]{data/exercise_output/ex_5_10/track_1_trajectories.eps}

\includegraphics[width=\textwidth]{data/exercise_output/ex_5_10/track_2_sample_trajectory.eps}
\end{document}


\clearpage
\section{Temporal-Difference Learning}

