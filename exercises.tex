\documentclass[a4paper, oneside, 11pt]{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{upquote}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=blue, citecolor=black]{hyperref}

\setlength{\parindent}{0cm}
\newcommand\Rule{\noindent\makebox[\textwidth]{\rule{\textwidth}{0.5pt}}}

\newcommand\RepoAddress{https://github.com/brynhayder/reinforcement_learning_an_introduction}

\newcommand\ProgrammingExercise{This is a programming exercise. For the relevant code/notebook please see \href{\RepoAddress{}}{the repo}.}

\newcommand\argmax{\operatorname*{argmax}}
\newcommand\argmin{\operatorname*{argmin}}

\newcommand\Epi{\mathbb{E}_\pi}

\renewcommand\P{\mathbb{P}}
\renewcommand{\familydefault}{\sfdefault}

\begin{document}

{\huge Exercises} \hfill {\huge Reinforcement Learning: An Introduction}\\
\Rule\\
\tableofcontents
\mbox{}\\
\Rule
\mbox{}\\


\setcounter{secnumdepth}{2}
% You can use the exam package to make some of this a bit easier.

\clearpage
\vfill
\thispagestyle{empty}
\begin{center}
    \emph{Code for exercises can be found at \href{\RepoAddress{}}{github.com/brynhayder/reinforcement\_learning\_an\_introduction}}\\[2cm]
Note that equation numbers in questions will refer to the original text.
\vfill
\end{center}
\clearpage
\section{Introduction}
\subsection{Exercise 1.1: Self-Play}
\subsubsection{Q}
Suppose, instead of playing against a random opponent, the reinforcement learning algorithm described above played against itself, with both sides learning. What do you think would happen in this case? Would it learn a different policy for selecting moves?

\subsubsection{A}
\begin{itemize}
    \item Would learn a different policy than playing a fixed opponent since the opponent would also be changing in this case.
    \item May not be able to learn an optimal strategy as the opponent keeps changing also.
    \item Could get stuck in loops.
    \item Policy could remain static since on average they would draw each iteration.
\end{itemize}

\subsection{Exercise 1.2: Symmetries}
\subsubsection{Q}
Many tic-tac-toe positions appear different but are really the same because of symmetries. How might we amend the learning process described above to take advantage of this? In what ways would this change improve the learning process? Now think again. Suppose the opponent did not take advantage of symmetries. In that case, should we? Is it true, then, that symmetrically equivalent positions should necessarily have the same value?

\subsubsection{A}
\begin{itemize}
    \item We could label the states as unique up to symmetries so that our search space is smaller, this way we will get a better estimate of optimal play.
    \item If we are playing an opponent who does not take symmetries into account when they are playing then we should not label the states as the same since the opponent is part of the environment and the environment is not the same in those states.
\end{itemize}

\subsection{Exercise 1.3: Greedy Play}
\subsubsection{Q}
Suppose the reinforcement learning player was greedy, that is, it always played the move that brought it to the position that it rated the best. Might it learn to play better, or worse, than a nongreedy player? What problems might occur

\subsubsection{A}
\begin{itemize}
    \item The greedy player will not explore, so will in general perform worse than the non-greedy player
    \item If the greedy player had a perfect estimate of the value of states then this would be fine.
\end{itemize}

\subsection{Exercise 1.4: Learning from Exploration}
\subsubsection{Q}
Suppose learning updates occurred after all moves, including exploratory moves. If the step-size parameter is appropriately reduced over time (but not the tendency to explore), then the state values would converge to a set of probabilities. What are the two sets of probabilities computed when we do, and when we do not, learn from exploratory moves? Assuming that we do continue to make exploratory moves, which set of probabilities might be better to learn? Which would result in more wins?
\subsubsection{A}
I think that an estimate for the probability of the state producing a win should be based on the optimal moves from that state.
\begin{itemize}
    \item The one in which we only record the optimal moves is the probability of our optimal agent winning. If we include exploration then this is the probability of the training agent winning.
    \item Better to learn the probability of winning with no exploration since this is how the agent will perform in real time play.
    \item Updating from optimal moves only will increase probability of winning.
\end{itemize}

\subsection{Exercise 1.5: Other Improvements}
\subsubsection{Q}
Can you think of other ways to improve the reinforcement learning player? Can you think of any better way to solve the tic-tac-toe problem as posed?

\subsubsection{A}
I'm not too sure here...
\begin{itemize}
    \item We could rank the draws as better than the losses.
    \item We might like to try running multiple iterations of games before updating our weights as this might give a better estimate.
\end{itemize}



\section{Multi-armed Bandits}
\subsection{Exercise 2.1}
\subsubsection{Q}
In $\varepsilon$-greedy action selection, for the case of two actions and $\varepsilon = 0.5$, what is the probability that the greedy action is selected?

\subsubsection{A}
0.5.


\subsection{Exercise 2.2: Bandit example}
\subsubsection{Q}
Consider a $k$-armed bandit problem with $k = 4$ actions, denoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using $\varepsilon$-greedy action selection, sample-average action-value estimates, and initial estimates of $Q_1(a) = 0$, for all $a$. Suppose the initial sequence of actions and rewards is $A_1 = 1$, $R_1 =1$, $A_2 =2$, $R_2 =1$, $A_3 =2$, $R_3 =2$, $A_4 =2$, $R_4 =2$, $A_5 =3$, $R_5 =0$. On some of these time steps the $\varepsilon$ case may have occurred, causing an action to be selected at random. On which time steps did this definitely occur? On which time steps could this possibly have occurred?

\subsubsection{A}
$A_2$ and $A_5$ were definitely exploratory. Any of the others \emph{could} have been exploratory.


\subsection{Exercise 2.3}
\subsubsection{Q}
In the comparison shown in Figure 2.2, which method will perform best in the long run in terms of cumulative reward and probability of selecting the best action? How much better will it be? Express your answer quantitatively.

\subsubsection{A}
The $\varepsilon = 0.01$ will perform better because in both cases as $t \to \infty$ we have $Q_t \to q_*$. The total reward and probability of choosing the optimal action will therefore be 10 times larger in this case than for $\varepsilon = 0.1$.


\subsection{Exercise 2.4}
\label{ex:2.4}
\subsubsection{Q}
If the step-size parameters, $\alpha_n$, are not constant, then the estimate $Q_n$ is a weighted average of previously received rewards with a weighting different from that given by (2.6). What is the weighting on each prior reward for the general case, analogous to (2.6), in terms of the sequence of step-size parameters?

\subsubsection{A}
Let $\alpha_0 = 1$, then 
\begin{equation}
    Q_{n + 1} = \left(\prod_{i=1}^n (1 - \alpha_i) \right) Q_1 + \sum_{i = 1}^{n}  \alpha_{i} R_{i} \prod_{k = i + 1}^n
(1 - \alpha_k).
\end{equation}
Where $\prod_{i=x}^y f(i) \doteq 1$ if $x > y$.

\subsection{Exercise 2.5 (programming)}
\subsubsection{Q}
Design and conduct an experiment to demonstrate the difficulties that sample-average methods have for non-stationary problems. Use a modified version of the 10-armed testbed in which all the $q_*(a)$ start out equal and then take independent random walks (say by adding a normally distributed increment with mean zero and standard deviation 0.01 to all the $q_*(a)$ on each step). Prepare plots like Figure 2.2 for an action-value method using sample averages, incrementally computed, and another action-value method using a constant step-size parameter, $\alpha$ = 0.1. Use $\varepsilon$ = 0.1 and longer runs, say of 10,000 steps.

\subsubsection{A}
\ProgrammingExercise

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_5/learning_curve.png}


\subsection{Exercise 2.6: Mysterious Values}
\subsubsection{Q}
The results shown in Figure 2.3 should be quite reliable because they are averages over 2000 individual, randomly chosen 10-armed bandit tasks. Why, then, are there oscillations and spikes in the early part of the curve for the optimistic method? In other words, what might make this method perform particularly better or worse, on average, on particular early steps?

\subsubsection{A}
At some point after step 10, the agent will find the optimal value. It will then choose this value greedily. The small step-size parameter (small relative to the initialisation value of 5) means that the estimate of the optimal value will converge slowly towards its true value.\\

It is likely that this true value is less than 5. This means that, due to the small step size, one of the sub-optimal actions will still have a value close to 5. Thus, at some point, the agent begins to act sub-optimally again.

\subsection{Exercise 2.7: Unbiased Constant Step Trick}
\subsubsection{Q}
In most of this chapter we have used sample averages to estimate action values because sample averages do not produce the initial bias that constant step sizes do (see the analysis in (2.6)). However, sample averages are not a completely satisfactory solution because they may perform poorly on non-stationary problems. Is it possible to avoid the bias of constant step sizes while retaining their advantages on non-stationary problems? One way is to use a step size of
\begin{equation}
    \beta_t \doteq \alpha / \bar{o}_t,
\end{equation}
where $\alpha > 0$ is a conventional constant step size and $\bar{o}_t$ is a trace of one that starts at 0:
\begin{equation}
    \bar{o}_{t+1} = \bar{o}_t + \alpha (1 - \bar{o}_t)
\end{equation}
for $t \geq 1$ and with $\bar{o}_1 \doteq \alpha$.\\

Carry out an analysis like that in (2.6) to show that $\beta_t$ is an exponential recency-weighted average \emph{without initial bias}. 

\subsubsection{A}
Consider the answer to \hyperref[ex:2.4]{Exercise 2.4}. There is no dependence of $Q_k$ on $Q_1$ for $k > 1$ since $\beta_1 = 1$. Now it remains to show that the weights in the remaining sum decrease as we look further into the past. That is
\begin{equation}
    w_i = \beta_i \prod_{k = i + 1}^{n} (1 - \beta_k)
\end{equation}
increases with $i$ for fixed n. For this, observe that
\begin{equation}
    \frac{w_{i+1}}{w_i} = \frac{\beta_{i+1}}{\beta_i(1 - \beta_{i + 1})} = \frac{1}{1 - \alpha} > 1
\end{equation} 
where we have assumed $\alpha < 1$. If $\alpha = 1$ then $\beta_t = 1 \,\, \forall \, t$.

\subsection{Exercise 2.8: UCB Spikes}
\subsubsection{Q}
In Figure 2.4 the UCB algorithm shows a distinct spike in performance on the 11th step. Why is this? Note that for your answer to be fully satisfactory it must explain both why the reward increases on the 11th step and why it decreases on the subsequent steps. Hint: if $c = 1$, then the spike is less prominent.

\subsubsection{A}
In the first 10 steps the agent cycles through all of the actions because when $N_t(a) = 0$ then $a$ is considered maximal. On the 11th step the agent will most often then choose greedily. The agent will continue to choose greedily until $\mathrm{ln}(t)$ overtakes $N_t(a)$ for one of the other actions, in which case the agent begins to explore again hence reducing rewards.\\

Note that, in the long run, $N_t = O(t)$ and $\mathrm{ln}(t) / t \to 1$. So this agent is `asymptotically greedy'.


\subsection{Exercise 2.9}
\subsubsection{Q}
Show that in the case of two actions, the soft-max distribution is the same as that given by the logistic, or sigmoid, function often used in statistics and artificial neural networks.

\subsubsection{A}
Let the two actions be denoted by 0 and 1. Now
\begin{equation}
    \P{}(A_t = 1) = \frac{e^{H_t(1)}}{e^{H_t(1)} + e^{H_t(0)}} = \frac{1}{1 + e^{-x}}, 
\end{equation}
where $x = H_t(1) - H_t(0)$ is the relative preference of 1 over 0.

\subsection{Exercise 2.10}
\subsubsection{Q}
Suppose you face a 2-armed bandit task whose true action values change randomly from time step to time step. Specifically, suppose that, for any time step, the true values of actions 1 and 2 are respectively 0.1 and 0.2 with probability 0.5 (case A), and 0.9 and 0.8 with probability 0.5 (case B). If you are not able to tell which case you face at any step, what is the best expectation of success you can achieve and how should you behave to achieve it? Now suppose that on each step you are told whether you are facing case A or case B (although you still don’t know the true action values). This is an associative search task. What is the best expectation of success you can achieve in this task, and how should you behave to achieve it?

\subsubsection{A}
I assume the rewards are stationary.\\

One should choose the action with the highest expected reward. In the first case, both action 1 and 2 have expected value of 0.5, so it doesn't matter which you pick.\\

In the second case one should run a normal bandit method separately on each colour. The expected reward from identifying the optimal actions in each case is 0.55.

\subsection{Exercise 2.11 (programming)}
\subsubsection{Q}
Make a figure analogous to Figure 2.6 for the non-stationary case outlined in Exercise 2.5. Include the constant-step-size $\varepsilon$-greedy algorithm with $\alpha=0.1$. Use runs of 200,000 steps and, as a performance measure for each algorithm and parameter setting, use the average reward over the last 100,000 steps.

\subsubsection{A}
\ProgrammingExercise

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_11/action_values.png}

\includegraphics[width=\textwidth]{data/exercise_output/ex_2_11/parameter_study.png}


\section{Finite Markov Decision Processes}

\subsection{Exercise 3.1}
\subsubsection{Q}
Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as \emph{different} from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.

\subsubsection{A}
\begin{enumerate}
    \item Simple example is a robot that hoovers a room. The state can be how much dust there is on the ground and where the robot is (including it's orientation). Actions can be to move and hoover. The reward can be the amount by which it reduces dust in the room on that action. This is Markov because all that is important from the previous state to the future is where the dust is left (maybe also how much).
    \item Outlandish example is a football coach. Actions are playing strategies. Rewards are goals. State is current score, team fitness, etc..
    \item financial trader. State is their current holdings on an asset. Reward is money from a trade. Actions are buy/sell. Maybe not markov because the environment may change predictably based on information from multiple steps ago.
\end{enumerate}

\subsection{Exercise 3.2}
\subsubsection{Q}
Is the MDP framework adequate to usefully represent \emph{all} goal-directed learning tasks? Can you think of any clear exceptions?

\subsubsection{A}
The main thing about the MDP is that Markov property. There are tasks where this does not hold. For instance, in Poker, the previous states will determine what is in the deck and what is not. This does not obey Markov property.

\subsection{Exercise 3.3}
\subsubsection{Q}
Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of \emph{where} to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice?

\subsubsection{A}

\begin{itemize}
    \item The natural distinction depends on the task. If the task is to go from one location to another, the actions might be in terms of directing the car and altering its speed. 
    \item This also depends on what we consider to be the decision making part here. If we consider the decisions to be made by the brain of a human driving, then their physical body will form part of the environment -- if they break their leg then it will effect their goal.
    \item Actions have to be things that the agent can actually control. Take the example of an autonomous vehicle. One might consider that the agent has complete control over the car's break, accelerator and steering wheel. These operations would form the actions for the agent. 
\end{itemize}


\subsection{Exercise 3.4}
\subsubsection{Q}
Give a table analogous to the one in Example 3.3, but for $p(s', r| s, a)$. It should have columns $s, a, s', r$, and a row for every 4-tuple for which $p(s', r | s, a) > 0$.

\subsubsection{A}
\begin{table}[h!]
\centering
\begin{tabular}{ll|ll|c}
    $s$ & $a$ & $s'$ & $r$ & $p(s', r | s, a)$ \\
    \hline
     \texttt{high}& \texttt{search}&  \texttt{high}&  $r_{\texttt{search}}$& $\alpha$  \\
     \texttt{high}& \texttt{search}&  \texttt{low}&   $r_{\texttt{search}}$& $1 - \alpha$ \\
     \texttt{high}& \texttt{wait}&  \texttt{high}&   $r_{\texttt{wait}}$&  $1$ \\
     \texttt{low}& \texttt{recharge}&   \texttt{high}&    $0$&   $1$\\
     \texttt{low}& \texttt{search}&  \texttt{high}&  $-3$& $1 - \beta$  \\
     \texttt{low}& \texttt{search}&  \texttt{low}&   $r_{\texttt{search}}$& $\beta $\\
     \texttt{low}& \texttt{wait}&  \texttt{low}&   $r_{\texttt{wait}}$&  $1$
\end{tabular}


\caption{Transition table}
\label{table:3.4}
\end{table}


\subsection{Exercise 3.5}
\subsubsection{Q}
The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3)

\subsubsection{A}
\begin{equation}
    \sum_{s' \in \mathcal{S}^{+}} \sum_{r \in \mathcal{R}} p(s', r | s, a) = 1
\end{equation}


\subsection{Exercise 3.6}
\subsubsection{Q}
Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for $-1$ upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

\subsubsection{A}
Note that the pole will fall eventually with probability 1
\begin{equation}
    G_t = - \gamma^{T - t}.
\end{equation}
Whereas in the continuing case the value is
\begin{equation}
    G_t = - \sum_{k \in \mathcal{K}} \gamma^{k - t},
\end{equation}
where $\mathcal{K}$ is the set of times after $t$ at which the pole falls over.

\subsection{Exercise 3.7}
\subsubsection{Q}
Imagine that you are designing a robot to run a maze. You decide to give it a reward of $+1$ for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes--the successive runs through the maze--so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

\subsubsection{A}
If the agent keeps going randomly, it will reach the end of the maze with probability 1, so the value of $G$ under most strategies is $1$. What you actually want is for the thing to leave the maze as quickly as possible.\\

Note also that there are some instances in which the agent might just get stuck in a loop. You would have to impose another rule to put the agent into a terminal state here.

\subsection{Exercise 3.8}
\subsubsection{Q}
Suppose $\gamma = 0.5$ and the following sequence of rewards is received $R_1=-1$, $R_2 =2$,$R_3 =6$,$R_4 =3$, and $R_5 =2$, with $T =5$. What are $G_0$, $G_1$, $\dots$, $G_5$? Hint: Work backwards.

\subsubsection{A}
$G_0 = 2$, $G_1 = 3$, $G_2 = 2$, $G_3 = \frac12$, $G_4 = \frac18$, $G_5=0$.

\subsection{Exercise 3.9}
\subsubsection{Q}
Suppose $\gamma = 0.9$ and the reward sequence is $R_1 = 2$ followed by an infinite sequence of $7$s. What are $G_1$ and $G_0$?
\subsubsection{A}
\begin{align}
    G_1 &= 7 \frac{\gamma}{1 - \gamma} \\
    G_0 &= 2 + 7 \frac{\gamma}{1 - \gamma}
\end{align}


\subsection{Exercise 3.10}
\subsubsection{Q}
Prove (3.10).

\subsubsection{A}
Take $r \in \mathbb{C}$ with $|r| < 1$, then
\begin{align*}
    S_N &\doteq \sum_{i=0}^N r^i \\
    rS_N - S_N &= r^{N+1} - 1 \\
    S_N &= \frac{1 - r^{N+1}}{1 - r} \\
    S &\doteq \lim_{N \to \infty} S_N = \frac{1}{1 - r}
\end{align*}

\subsection{Exercise 3.11}
\subsubsection{Q}
If the current state is $S_t$, and actions are selected according to stochastic policy $\pi$, then what is the expectation of $R_{t+1}$ in terms of $\pi$ and the four-argument function $p$ (3.2)?

\subsubsection{A}
\begin{equation}
    \Epi{} [R_{t+1} | S_t = s] = \sum_{a} \pi(a | s) \sum_{s', r} r p(s', r | s, a)
\end{equation}


\subsection{Exercise 3.12}
\subsubsection{Q}
The Bellman equation (3.14) must hold for each state for the value function $v_\pi$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at $+0.7$, with respect to its four neighbouring states, valued at $+2.3$, $+0.4$, $-0.4$, and $+0.7$. (These numbers are accurate only to one decimal place.)

\subsubsection{A}
\begin{align*}
    v_\pi(\mathtt{center}) &\doteq \sum_a \pi(a | s) \sum_{s', r} p(s', r | s, a)[r + \gamma v_\pi(s')] \\
    & = \frac14 \times 0.9 \times \sum_{s'} v_\pi(s')\\
    & = \frac14 \times 0.9 \times 3.0 \\
    & = 0.675
\end{align*}


\subsection{Exercise 3.13}
\subsubsection{Q}
What is the Bellman equation for action values, that is, for $q_\pi$? It must give the action value $q_\pi(s, a)$ in terms of the action values, $q_\pi(s',a')$, of possible successors to the state–action pair $(s,a)$. Hint: the backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values.

\subsubsection{A}
\begin{align*}
    q_\pi &\doteq \Epi[G_t | S_t=s, A_t=a]\\
          &= \Epi[R_{t+1}| S_t=s, A_t=a] + \gamma \Epi[G_{t+1} |S_t = s, A_t = a] \\
          &= \sum_{s', r} p(s', r|s, a)r + \gamma \sum_{s', r} p(s', r|s, a) \sum_{a'}\pi(a'|s')\Epi[G_{t+1}| S_{t+1}=s', A_{t+1}=a'] \\
          &= \sum_{s', r} p(s', r| s, a) [r + \gamma \sum_{a'}\pi(a'|s')q_\pi(s', a')]
\end{align*}


\subsection{Exercise 3.14}
\subsubsection{Q}
In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant $c$ to all the rewards adds a constant, $v_c$, to the values of all states, and thus does not affect the relative values of any states under any policies. What is $v_c$ in terms of $c$ and $\gamma$?

\subsubsection{A}
We choose actions by relative (additive) values. Add $c$ to all states and 
\begin{equation}
    G_t \mapsto G_t + \frac{c}{1 - \gamma}
\end{equation}
so relative values unchanged.\\

$v_c = \frac{c}{1 - \gamma}$

\subsection{Exercise 3.15}
\subsubsection{Q}
Now consider adding a constant $c$ to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.

\subsubsection{A}
Let terminal time be $T$. In this case we have 
\begin{equation}
    G_t \mapsto G_t + c\frac{1 - \gamma^T}{1 - \gamma},
\end{equation}
so if the agent can procrastinate termination, then all else being equal it will increase $v_\pi$.\\

Suppose that we have an episodic task with one state $S$ and two actions $A_0, A_1$. $A_0$ takes agent to terminal state with reward $1$, while $A_1$ takes agent back to $S$ with reward $0$. In this case the agent should terminate to maximise reward. \\ 

If we add $1$ to each reward then the return for doing $A_1$ forever is $\frac{1}{1-\gamma}$. Which can be bigger than $2$ if we choose a discount factor smaller than $\frac12$.

\subsection{Exercise 3.16}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_16_question.png}
 
\subsubsection{A}
\begin{align*}
    v_\pi(s) & = \Epi[q_\pi(S_t, A_t) |S_t = s, A_t = a] \\ 
          & = \sum_a \pi(a|s) q_\pi(s, a)
\end{align*}

\subsection{Exercise 3.17}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_17_question.png}
 
\subsubsection{A} 
\begin{align*}
    q_\pi(s, a) & = \Epi[R_{t+1} + v_\pi(s') |S_t = s, A_t = a] \\ 
          & = \sum_{s', r} p(s', r|s, a) [r + v_\pi(s')]
\end{align*}


\subsection{Exercise 3.18}
\subsubsection{Q}
Draw or describe the optimal state-value function for the golf example.

\subsubsection{A}
(Using the pictures.) Optimal state value gives values according to \texttt{driver} when off the green, then according to \texttt{putter} on the green.\\

Optimal policy is to use \texttt{driver} when off green and \texttt{putter} when on green.
 
 
\subsection{Exercise 3.19}
\subsubsection{Q}
 Draw or describe the contours of the optimal action-value function for putting, $q_*(s, \mathtt{putter})$, for the golf example.

\subsubsection{A}
Should be the same as the value for the policy that always uses the \texttt{putter}.

\includegraphics[width=\textwidth]{data/exercise_output/ex_3_19/ex_3_19_answer.png}


\subsection{Exercise 3.20}
\subsubsection{Q}
\includegraphics[width=\textwidth]{data/exercise_questions/ex_3_20_question.png}
 
\subsubsection{A}
When $\gamma=0$, $v_{\pi_{\mathtt{left}}}$ is optimal. When $\gamma = 0.5$, they are both optimal. When $\gamma=0.9$, $v_{\pi_{\mathtt{right}}}$ is optimal.

\subsection{Exercise 3.21}
\subsubsection{Q}
Give the Bellman equation for $q_*$ for the recycling robot.

\subsubsection{A}
This is just writing out the equation below, filling in the values given in the robot example.
\begin{equation}
    q_*(s, a) = \sum_{s', r} p(s', r | s, a)[r + \gamma \max_{a'} q_*(s', a')]
\end{equation}

\subsection{Exercise 3.22}
\subsubsection{Q}
Figure 3.5 gives the optimal value of the best state of the gridworld as $24.4$, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places.

\subsubsection{A}
All actions tak the agent to $A'$ with reward $10$. We can see that $\gamma = 16.0 / 17.8 = 0.9$. This means that
\begin{equation}
    v = 10 + 16 \times 0.9 = 24.4.
\end{equation}
This is using the following framework
\begin{equation}
    v_*(s) = \max_{a}\sum_{s', r} p (s', r| s, a)[r + \gamma v_*(s')].
\end{equation}


\subsection{Exercise 3.23}
\subsubsection{Q}
Give an equation for $v_*$ in terms of $q_*$

\subsubsection{A}
\begin{equation}
    v_*(s) = \sum_a \pi^*(a | s) q_*(s, a)
\end{equation}


\subsection{Exercise 3.24}
\subsubsection{Q}
Give an equation for $q_*$ in terms of $v_*$ and the world's dynamics $p(s', r| s, a)$.

\subsubsection{A}
\begin{align}
    q_* (s, a) &= \mathbb{E} [ R_{t+1} + \gamma v_*(S_{t+1}) | S_t = s, A_t = a] \\
    &= \sum_{s', r} (r + \gamma v_*(s'))p(s', r | s, a)
\end{align} 


\subsection{Exercise 3.25}
\subsubsection{Q}
Give an equation for $\pi_*$ in terms of $q_*$.

\subsubsection{A}
This is just any policy that acts greedily w.r.t. the optimal action-value function.
\begin{equation}
    \pi_* (a|s) = \frac{\mathds{1}\{a = \argmax_{a'} q_*(a', s)\}}{\sum_{a} \mathds{1}\{a = \argmax_{a'}q_*(a', s)\}}
\end{equation} 

\subsection{Exercise 3.26}
\subsubsection{Q}
Give an equation for $\pi_*$ in terms of $v_*$ and the world's dynamics $p(s', r| s, a)$.

\subsubsection{A}
This is just the answer to 3.25 with the answer to 3.24 substituted in for $q_*$.


\section{Dynamic Programming}



\end{document}